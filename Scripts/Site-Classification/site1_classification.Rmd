---
title: "Site 1 Classification"
author: "Natalie"
date: "2024-08-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages, message=F}
library(mlr3)
library(mlr3learners)
library(mlr3spatial)
library(terra)
library(sf)
library(ggplot2)
library(tidyterra)
library(here)
```
# Import Data
Import raster and training polygons
```{r}
site1 <- rast(here("Data", "site1", "site1.tif"))
site1_poly <- vect(here("Data", "site1", "site1_poly.shp"))
```

# Prepare data for model training
Generate 200 random points within each polygon
```{r}
# Create empty spatvector to store points
site1_pts <- vect()
```

```{r}
#Loop through each polygon in the shapefile
set.seed(101)
for (i in 1:nrow(site1_poly)) {
  single_poly <- site1_poly[i, ]
  randomPts <- spatSample(single_poly, 200, method = "random")
  site1_pts <- rbind(site1_pts, randomPts)
}
print(site1_pts)
```
```{r}
#Save points as a shapefile
#writeVector(site1_randomPts, here("Data", "site1", "site1_pts.shp"), filetype = "ESRI Shapefile", overwrite=T) #can omit
```
Note: it is not necessary to save the point shapefile to your computer. I only do it so I have a copy of the files for storage purposes. 

Extract RGB values from the raster at the training points
```{r}
#Rename raster layers
names(site1) <- c("red", "green", "blue")
```

```{r}
#Extract raster values at the points
site1_extracted <- extract(site1, site1_pts)
```

```{r}
#Combine raster values with the point data
site1_pts_rgb <- cbind(site1_pts, site1_extracted)
sum(is.na(site1_pts_rgb))
#writeVector(site1_pts_rgb, here("Data", "site1", "site1_pts_rgb.gpkg"), filetype = "GPKG", overwrite = T) #can omit
```

#Model training
Import geopackage
```{r}
# The geopackage must be imported using the "read_sf" function for it to work with the classifier
site1_gpkg <- read_sf(here("Data", "site1", "site1_pts_rgb.gpkg"), stringsAsFactors = T)
site1_gpkg <- site1_gpkg[, c("className", "red", "green", "blue", "geom")]
site1_gpkg
sum(is.na(site1_gpkg))
```

Create mlr3 task
```{r}
site1_tsk <- as_task_classif_st(site1_gpkg, target = "className")
```

Partition the data into 70% training and 30% testing.
```{r}
set.seed(100)
site1_split <- partition(site1_tsk, ratio = 0.7)
```

Create a random forest learner 
```{r}
site1_lrn <- lrn("classif.ranger")
```

Train the model
```{r}
site1_train <- site1_lrn$train(site1_tsk, site1_split$train)
```

Use the test set to obtain the model's accuracy
```{r}
site1_test <- site1_lrn$predict(site1_tsk, site1_split$test)
site1_test$score(msr("classif.acc"))
```

# Classifying the site and obtaining area and percent coverage 

Apply the learner to the entire raster. This step can take a VERY long time depending on how big your raster is. 
```{r}
site1_classified <- predict_spatial(site1, site1_lrn)
```
Save the classified raster (optional)
```{r}
writeRaster(site1_classified, here("Data", "site1", "site1_classified.tif"), overwrite=T)
```

Calculate the area of each class
```{r}
# Obtain the area of each pixel in sq meters
pixel_res <- res(site1_classified)[1]
pixel_area <- pixel_res^2
```

```{r}
# Calculate how many pixels there are in each class and store it in a dataframe
class_frequencies <- freq(site1_classified)
class_frequencies_df <- as.data.frame(class_frequencies)
```

```{r}
#calculate area of each class
class_frequencies_df$area_m2 <- class_frequencies_df$count * pixel_area
```

Calculate the percent coverage of each class
```{r}
#get the total area of the raster
total_area_m2 <- sum(class_frequencies_df$area_m2)
```

```{r}
#calculate percent coverage
class_frequencies_df$percent_coverage <- (class_frequencies_df$area_m2 / total_area_m2) * 100
```

```{r}
#rename class values
class_frequencies_df$class <- c("antelope brush", "vegetation", "ground")
```

```{r}
#display area and percent coverage
print(class_frequencies_df)
```

